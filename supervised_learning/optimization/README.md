# Optimization - Machine Learning

This directory contains various Python scripts that implement different optimization algorithms and techniques used in machine learning. These include normalization, shuffling data, mini-batch gradient descent, moving average, momentum optimization, RMSProp, Adam optimization, learning rate decay, and batch normalization.

## Synopsis

Optimization is a key aspect of training machine learning models effectively. The scripts in this directory demonstrate various techniques used to optimize the training process and improve the performance of the models.

## File Descriptions

1. [0-norm_constants.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/0-norm_constants.py): Calculates the normalization (standardization) constants of a matrix.
2. [1-normalize.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/1-normalize.py): Normalizes (standardizes) a matrix.
3. [2-shuffle_data.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/2-shuffle_data.py): Shuffles the data points in two matrices the same way.
4. [3-mini_batch.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/3-mini_batch.py): Trains a loaded neural network model using mini-batch gradient descent.
5. [4-moving_average.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/4-moving_average.py): Calculates the weighted moving average of a data set.
6. [5-momentum.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/5-momentum.py): Updates a variable using the gradient descent with momentum optimization algorithm.
7. [6-momentum.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/6-momentum.py): Creates the training operation for a neural network in tensorflow using the gradient descent with momentum optimization algorithm.
8. [7-RMSProp.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/7-RMSProp.py): Updates a variable using the RMSProp optimization algorithm.
9. [8-RMSProp.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/8-RMSProp.py): Creates the training operation for a neural network in tensorflow using the RMSProp optimization algorithm.
10. [9-Adam.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/9-Adam.py): Updates a variable in place using the Adam optimization algorithm.
11. [10-Adam.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/10-Adam.py): Creates the training operation for a neural network in tensorflow using the Adam optimization algorithm.
12. [11-learning_rate_decay.py](https://github.com/ZeroDayPoke/holbertonschool-machine_learning/tree/main/supervised_learning/optimization/11-learning_rate_decay.py): Updates the learning rate using inverse time decay in numpy.

## Author

Chris Stamper - [ZeroDayPoke](https://github.com/ZeroDayPoke)
